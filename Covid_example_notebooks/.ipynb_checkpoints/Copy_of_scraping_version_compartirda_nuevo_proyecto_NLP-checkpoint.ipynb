{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pxEEKZq5eysI"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GnViQoC3648l"
   },
   "source": [
    "Actualmente este .ipynb es inservible, la librería GetOldTweets3 está temporalmente restringida por Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 30679,
     "status": "ok",
     "timestamp": 1643980591058,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "-Z6w4BDQeLca",
    "outputId": "b6b1873c-b940-4e30-85a0-b0926248b483"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted at /content/gdrive\n"
     ]
    }
   ],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/gdrive', force_remount=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 4571,
     "status": "ok",
     "timestamp": 1643980820077,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "32zs2toRNBCz",
    "outputId": "0de838e8-ed24-4887-9753-82e9998833b1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package vader_lexicon to /root/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n",
      "Collecting GetOldTweets3\n",
      "  Downloading GetOldTweets3-0.0.11-py3-none-any.whl (13 kB)\n",
      "Requirement already satisfied: lxml>=3.5.0 in /usr/local/lib/python3.7/dist-packages (from GetOldTweets3) (4.2.6)\n",
      "Collecting pyquery>=1.2.10\n",
      "  Downloading pyquery-1.4.3-py3-none-any.whl (22 kB)\n",
      "Collecting cssselect>0.7.9\n",
      "  Downloading cssselect-1.1.0-py2.py3-none-any.whl (16 kB)\n",
      "Installing collected packages: cssselect, pyquery, GetOldTweets3\n",
      "Successfully installed GetOldTweets3-0.0.11 cssselect-1.1.0 pyquery-1.4.3\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "#paquetes y librerias necesarias\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import tqdm\n",
    "import time\n",
    "from PIL import Image\n",
    "from wordcloud import WordCloud\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.datasets import mnist\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Conv2D, MaxPooling2D, Flatten\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from keras import regularizers\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from IPython.display import clear_output\n",
    "from dateutil.parser import parse\n",
    "from datetime import datetime\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "try:\n",
    "  import GetOldTweets3 as got\n",
    "except ModuleNotFoundError:\n",
    "  !pip install GetOldTweets3\n",
    "  \n",
    "import datetime\n",
    "from textblob import TextBlob\n",
    "from IPython.display import clear_output\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "\n",
    "\n",
    "#from datetime import datetime\n",
    "#from dateutil.parser import parse\n",
    "#from googletrans import Translator\n",
    "\n",
    "\n",
    "%matplotlib inline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jzmlwiZHO7Kh"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_3kF5Bdn3Ao1"
   },
   "source": [
    "## Creacion del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 223,
     "status": "ok",
     "timestamp": 1643980861091,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "JwcpO0XoNCC_"
   },
   "outputs": [],
   "source": [
    "def listmaker(username,n):\n",
    "    lista = [username] * n\n",
    "    return lista\n",
    "\n",
    "#Funcion para la obtencion de los tweets\n",
    "def robatweets(username,fecha_ini,fecha_fin):\n",
    "\n",
    "  tweetCriteria = got.manager.TweetCriteria().setUsername(username).setSince(fecha_ini).setUntil(fecha_fin)\n",
    "\n",
    "  tweets = got.manager.TweetManager.getTweets(tweetCriteria)\n",
    "\n",
    "  fechas = [datetime.date(tweet.date.year,tweet.date.month,tweet.date.day).isoformat() for tweet in tweets]\n",
    "  todos_tweets = [tweet.text for tweet in tweets]\n",
    "  nombre = listmaker(username = username,n=len(fechas))\n",
    "  return [todos_tweets,fechas,nombre]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 205,
     "status": "ok",
     "timestamp": 1643980865542,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "CAcERT_yXWHT"
   },
   "outputs": [],
   "source": [
    "\n",
    "fecha_ini = \"2016-01-01\"\n",
    "fecha_fin = \"2020-06-06\"\n",
    "\n",
    "#listas VOX\n",
    "#Santi_ABASCAL = robatweets(username = '@Santi_ABASCAL',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#Jorgebuxade = robatweets(username ='@Jorgebuxade',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#vicpiedra = robatweets(username = '@vicpiedra',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#rromerovilches = robatweets(username = '@rromerovilches',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#Pablosaenzd = robatweets(username = '@Pablosaenzd',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#Vox_Molina = robatweets(username = '@Vox_Molina',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#MeerRocio = robatweets(username = '@MeerRocio',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#Igarrigavaz = robatweets(username = '@Igarrigavaz',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#jlsteeg = robatweets(username = '@jlsteeg',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#RuizSolas = robatweets(username = '@RuizSolas',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#pedro_fhz = robatweets(username = '@pedro_fhz',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "\n",
    "#listas Podemos\n",
    "#PabloIglesias = robatweets(username = '@PabloIglesias',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#MiguelUrban = robatweets(username = '@MiguelUrban',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#pilar_lima = robatweets(username = '@pilar_lima',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#anamarcelloana = robatweets(username = '@anamarcelloana',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#pbustinduy = robatweets(username = '@pbustinduy',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#Alber_Canarias = robatweets(username = '@Alber_Canarias',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#ionebelarra = robatweets(username = '@ionebelarra',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#MayoralRafa = robatweets(username = '@MayoralRafa',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#Julio_Rodr_ = robatweets(username = '@Julio_Rodr_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#VeraNoelia = robatweets(username = '@VeraNoelia',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#IreneMontero = robatweets(username = '@IreneMontero',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#pnique = robatweets(username = '@pnique',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#VicencNavarro = robatweets(username = '@VicencNavarro',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "\n",
    "#listas PSOE\n",
    "#sanchezcastejon = robatweets(username = '@sanchezcastejon',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#CristinaNarbona = robatweets(username = '@CristinaNarbona',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#Adrilastra = robatweets(username = '@Adrilastra',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#abalosmeco = robatweets(username = '@abalosmeco',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#santicl = robatweets(username = '@santicl',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#JoseantonioJun = robatweets(username = '@JoseantonioJun',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#patxilopez = robatweets(username = '@patxilopez',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#oscar_puente_ = robatweets(username = '@oscar_puente_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#gomezdcelis = robatweets(username = '@gomezdcelis',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "\n",
    "#listas PP\n",
    "#pablocasado_ = robatweets(username = '@pablocasado_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#JavierMaroto = robatweets(username = '@JavierMaroto',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#ja_nietob = robatweets(username = '@ja_nietob',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#AlmeidaPP_ = robatweets(username = '@AlmeidaPP_',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#sanchezcesar = robatweets(username = '@sanchezcesar',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#abeltran_ana = robatweets(username = '@abeltran_ana',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#javiermarquezsa = robatweets(username = '@javiermarquezsa',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#vicentetiradopp = robatweets(username = '@vicentetiradopp',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#DiegoCalvoPouso = robatweets(username = '@DiegoCalvoPouso',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "\n",
    "#listas Ciudadanos\n",
    "#InesArrimadas = robatweets(username = '@InesArrimadas',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#MarinaBS_Cs = robatweets(username = '@MarinaBS_Cs',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#CCuadradoCs = robatweets(username = '@CCuadradoCs',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#jmespejosaav = robatweets(username = '@jmespejosaav',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#joanmesquida62 = robatweets(username = '@joanmesquida62',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#MelisaRguezCs = robatweets(username = '@MelisaRguezCs',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#BalEdmundo = robatweets(username = '@BalEdmundo',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#Tonicanto1 = robatweets(username = '@Tonicanto1',fecha_ini = fecha_ini, fecha_fin = fecha_fin)\n",
    "#Lroldansu = robatweets(username = '@Lroldansu',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#Albert_Rivera = robatweets(username = '@Albert_Rivera',fecha_ini = fecha_ini, fecha_fin = fecha_fin) \n",
    "#ignacioaguado = robatweets(username = '@ignacioaguado',fecha_ini = fecha_ini, fecha_fin = fecha_fin)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 288
    },
    "executionInfo": {
     "elapsed": 221,
     "status": "error",
     "timestamp": 1643980869649,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "3I7pMQk0Xjwy",
    "outputId": "36995b56-71bf-43a4-cb68-90c56c146fe5"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-310f8c586849>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m     \u001b[0mVOX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSanti_ABASCAL\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mJorgebuxade\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mvicpiedra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrromerovilches\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mPablosaenzd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVox_Molina\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMeerRocio\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIgarrigavaz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mjlsteeg\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mRuizSolas\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m+\u001b[0m \u001b[0mpedro_fhz\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m     \u001b[0mPodemos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPabloIglesias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMiguelUrban\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpilar_lima\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0manamarcelloana\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpbustinduy\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mAlber_Canarias\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mionebelarra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mMayoralRafa\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mJulio_Rodr_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVeraNoelia\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mIreneMontero\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpnique\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mVicencNavarro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mPSOE\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msanchezcastejon\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mCristinaNarbona\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mAdrilastra\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mabalosmeco\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0msanticl\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mJoseantonioJun\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mpatxilopez\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0moscar_puente_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mgomezdcelis\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'Santi_ABASCAL' is not defined"
     ]
    }
   ],
   "source": [
    "#creamos las listas completas de los partidos, las cuales incluyen el texto, la fecha y el autor\n",
    "\n",
    "i=0\n",
    "VOX = []\n",
    "Podemos = []\n",
    "PSOE = []\n",
    "PP = []\n",
    "Ciudadanos = []\n",
    "\n",
    "for i in range(3):\n",
    "    VOX.append(Santi_ABASCAL[i] + Jorgebuxade[i] + vicpiedra[i] + rromerovilches[i] + Pablosaenzd[i] + Vox_Molina[i] + MeerRocio[i] + Igarrigavaz[i] + jlsteeg[i]  + RuizSolas[i]  + pedro_fhz[i]) \n",
    "    Podemos.append(PabloIglesias[i] + MiguelUrban[i] + pilar_lima[i] + anamarcelloana[i] + pbustinduy[i] + Alber_Canarias[i] + ionebelarra[i] + MayoralRafa[i] + Julio_Rodr_[i] + VeraNoelia[i] + IreneMontero[i] + pnique[i] + VicencNavarro[i]) \n",
    "    PSOE.append(sanchezcastejon[i] + CristinaNarbona[i] + Adrilastra[i] + abalosmeco[i] + santicl[i] + JoseantonioJun[i] + patxilopez[i] + oscar_puente_[i] + gomezdcelis[i])\n",
    "    PP.append(pablocasado_[i] + JavierMaroto[i] + ja_nietob[i] + AlmeidaPP_[i] + sanchezcesar[i] + abeltran_ana[i] + javiermarquezsa[i] + vicentetiradopp[i] + DiegoCalvoPouso[i])\n",
    "    Ciudadanos.append(InesArrimadas[i] + MarinaBS_Cs[i] + CCuadradoCs[i] + jmespejosaav[i] + joanmesquida62[i] + MelisaRguezCs[i] + BalEdmundo[i] + Tonicanto1[i] + Lroldansu[i] + Albert_Rivera[i] + ignacioaguado[i]) \n",
    "\n",
    "print(len(VOX[0]))\n",
    "print(len(Podemos[0]))\n",
    "print(len(PSOE[0]))\n",
    "print(len(PP[0]))\n",
    "print(len(Ciudadanos[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 201,
     "status": "ok",
     "timestamp": 1643980926933,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "i_7rMCfr5dck",
    "outputId": "039a4584-dcae-4848-a5b6-a879a76e0309"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "list"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(VOX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "a-g_Gzf7zBVl",
    "outputId": "c3c37c9b-fff3-48cf-f227-f3ad3bb1db21"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138203\n",
      "138203\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['#ComienzaElCambio en Andalucía tras cuatro décadas de gobiernos socialistas con @JuanMa_Moreno, nuevo presidente de la Junta. Es un honor ser Presidente del @PPopular en este momento histórico para España. #InvestiduraAnd #JuanmaPresidente',\n",
       " 'PP',\n",
       " 'OPO',\n",
       " '@pablocasado_',\n",
       " '2019-01-16']"
      ]
     },
     "execution_count": 337,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "VOX_complete_list = []\n",
    "Podemos_complete_list = []\n",
    "PSOE_complete_list = []\n",
    "PP_complete_list = []\n",
    "Ciudadanos_complete_list = []\n",
    "\n",
    "#preparamos para el CSV\n",
    "#Filtramos de manera que solo se queden los tweets con mas de 7 palabras. Por otro lado añadimos la formacion politica y su posición en el periodo estudiado: oposicion y gobierno\n",
    "for i in range(len(VOX[0])):\n",
    "  if len(VOX[0][i].split()) > 7:\n",
    "    VOX_complete_list.append([VOX[0][i],'VOX','OPO',VOX[2][i],VOX[1][i]])\n",
    "for i in range(len(Podemos[0])):\n",
    "  if len(Podemos[0][i].split()) > 7:\n",
    "    Podemos_complete_list.append([Podemos[0][i],'POD','GOV',Podemos[2][i],Podemos[1][i]])\n",
    "for i in range(len(PSOE[0])):\n",
    "  if len(PSOE[0][i].split()) > 7:\n",
    "    PSOE_complete_list.append([PSOE[0][i],'PSOE','GOV',PSOE[2][i],PSOE[1][i]])\n",
    "for i in range(len(PP[0])):\n",
    "  if len(PP[0][i].split()) > 7:\n",
    "    PP_complete_list.append([PP[0][i],'PP','OPO',PP[2][i],PP[1][i]]) \n",
    "for i in range(len(Ciudadanos[0])):\n",
    "  if len(Ciudadanos[0][i].split()) > 7:\n",
    "    Ciudadanos_complete_list.append([Ciudadanos[0][i],'CIU','OPO',Ciudadanos[2][i],Ciudadanos[1][i]])   \n",
    "\n",
    "\n",
    "lista_completa = VOX_complete_list + Podemos_complete_list + PSOE_complete_list + PP_complete_list + Ciudadanos_complete_list\n",
    "import random\n",
    "lista_completa_mezclada=random.sample(lista_completa,k=len(lista_completa))\n",
    "\n",
    "print(len(lista_completa))\n",
    "print(len(lista_completa_mezclada))\n",
    "\n",
    "lista_completa_mezclada[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fAFVnb6DgWDt"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"numero\", \"tweet\", \"nombre\",\"posicion\", \"cuenta\", \"fecha\"])\n",
    "    for i in range(len(lista_completa_mezclada)):\n",
    "        writer.writerow([i, lista_completa_mezclada[i][0], lista_completa_mezclada[i][1], lista_completa_mezclada[i][2],lista_completa_mezclada[i][3],lista_completa_mezclada[i][4]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "A1VZ5lebPoTd"
   },
   "source": [
    "## Simplificación Versión 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "id": "9KTXyqBTPb7O",
    "outputId": "675fb823-0c29-4de6-8855-4e1eda0f8091"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nombre</th>\n",
       "      <th>posicion</th>\n",
       "      <th>cuenta</th>\n",
       "      <th>fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Acabo de sumarme a la #HoradelPlaneta con @WWF...</td>\n",
       "      <td>PSOE</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@JoseantonioJun</td>\n",
       "      <td>2017-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Nos hemos quedado huérfanos de una fiscalía q...</td>\n",
       "      <td>POD</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@anamarcelloana</td>\n",
       "      <td>2016-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Acto Electoral en las #TorresdeCotillas Jueve...</td>\n",
       "      <td>VOX</td>\n",
       "      <td>OPO</td>\n",
       "      <td>@Vox_Molina</td>\n",
       "      <td>2019-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Para llevarse tan mal se entienden muy bien. P...</td>\n",
       "      <td>PSOE</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@sanchezcastejon</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#ComienzaElCambio en Andalucía tras cuatro déc...</td>\n",
       "      <td>PP</td>\n",
       "      <td>OPO</td>\n",
       "      <td>@pablocasado_</td>\n",
       "      <td>2019-01-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numero  ...       fecha\n",
       "0       0  ...  2017-03-22\n",
       "1       1  ...  2016-03-04\n",
       "2       2  ...  2019-11-05\n",
       "3       3  ...  2018-03-26\n",
       "4       4  ...  2019-01-16\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 339,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv')\n",
    "df.head(5) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "SFwTdDFeAKD6",
    "outputId": "3126d8e9-71dc-45b5-ce8e-e5a8327c9462"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/138203 [00:00<?, ?it/s]/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:39: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:40: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  0%|          | 4/138203 [00:00<1:01:09, 37.66it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   numero  ...       fecha\n",
      "0       0  ...  2017-03-22\n",
      "1       1  ...  2016-03-04\n",
      "2       2  ...  2019-11-05\n",
      "3       3  ...  2018-03-26\n",
      "4       4  ...  2019-01-16\n",
      "\n",
      "[5 rows x 6 columns]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 138203/138203 [47:14<00:00, 48.76it/s]\n"
     ]
    }
   ],
   "source": [
    "# Creamos la funcion para el procesado de datos\n",
    "def text_process(tex):\n",
    "    # 1. Eliminamos todas las cosas de marcacion: puntos, comas, etc\n",
    "    nopunct=[char for char in tex if char not in string.punctuation]\n",
    "    nopunct=''.join(nopunct)\n",
    "    \n",
    "    # 2. Lemmatisation: proceso de agrupacion de palabras segun la raiz. Por ejemplo\n",
    "    #    studing y study en el fondo son la misma palabra\n",
    "    \n",
    "    a=''\n",
    "    i=0\n",
    "    for i in range(len(nopunct.split())):\n",
    "        b=lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
    "        a=a+b+' '\n",
    "        \n",
    "    # 3. Eliminamos todas las palabras cortas e inutiles: a, an, the... que no nos\n",
    "    #   dicen absolutamente nada de la forma de escribir del autor (obviamente utilizamos el diccionario español)\n",
    "    return [word for word in a.split() if word.lower() not in stopwords.words('spanish')]\n",
    "    #return [word for word in nopunct.split() if word.lower() not in stopwords.words('spanish')]\n",
    "\n",
    "\n",
    "\n",
    "#df['tweet_limpios'] = df['tweet'].str.replace(\"[^a-zA-Z#áéíóúñ]\", \" \")\n",
    "#df['tweet_limpios'] = df['tweet_limpios'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "#df['tweet_limpios'] = text_process(df['tweet_limpios'])\n",
    "\n",
    "df['tweet_limpios'] = df['tweet'] #creamos una nueva columna en el df\n",
    "\n",
    "for i in tqdm.tqdm(range(len(df['tweet_limpios']))): #Ejecutamos el procesado de datos para cada tweet del df\n",
    "#for i in tqdm.tqdm(range(100)):\n",
    "    df['tweet_limpios'][i] = text_process(df['tweet_limpios'][i])\n",
    "    df['tweet_limpios'][i] = ' '.join(df['tweet_limpios'][i])\n",
    "    time.sleep(0.01) #Esto es para marcar el tiempo, que la funcion es lenta\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PEpmexBxAKS_"
   },
   "outputs": [],
   "source": [
    "#Finalmente añadimos año y mes a nuestro df\n",
    "\n",
    "from dateutil.parser import parse\n",
    "\n",
    "año = []\n",
    "mes = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    año.append(parse(df['fecha'][i]).year)\n",
    "    mes.append(parse(df['fecha'][i]).month)\n",
    "df['año'] = año\n",
    "df['mes'] = mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 289
    },
    "id": "BbIsoXUsXdoL",
    "outputId": "021f5832-fbf6-4959-d7e4-f7ff8d961220"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nombre</th>\n",
       "      <th>posicion</th>\n",
       "      <th>cuenta</th>\n",
       "      <th>fecha</th>\n",
       "      <th>tweet_limpios</th>\n",
       "      <th>año</th>\n",
       "      <th>mes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Acabo de sumarme a la #HoradelPlaneta con @WWF...</td>\n",
       "      <td>PSOE</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@JoseantonioJun</td>\n",
       "      <td>2017-03-22</td>\n",
       "      <td>Acabo sumarme HoradelPlaneta WWFespana ¿y http...</td>\n",
       "      <td>2017</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Nos hemos quedado huérfanos de una fiscalía q...</td>\n",
       "      <td>POD</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@anamarcelloana</td>\n",
       "      <td>2016-03-04</td>\n",
       "      <td>quedado huérfanos fiscalía defendido inter tod...</td>\n",
       "      <td>2016</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Acto Electoral en las #TorresdeCotillas Jueve...</td>\n",
       "      <td>VOX</td>\n",
       "      <td>OPO</td>\n",
       "      <td>@Vox_Molina</td>\n",
       "      <td>2019-11-05</td>\n",
       "      <td>Acto Electoral TorresdeCotillas Jueves 0711 Ca...</td>\n",
       "      <td>2019</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Para llevarse tan mal se entienden muy bien. P...</td>\n",
       "      <td>PSOE</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@sanchezcastejon</td>\n",
       "      <td>2018-03-26</td>\n",
       "      <td>llevarse tan mal entienden bien PP Cs vuelven ...</td>\n",
       "      <td>2018</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#ComienzaElCambio en Andalucía tras cuatro déc...</td>\n",
       "      <td>PP</td>\n",
       "      <td>OPO</td>\n",
       "      <td>@pablocasado_</td>\n",
       "      <td>2019-01-16</td>\n",
       "      <td>ComienzaElCambio Andalucía tras cuatro décadas...</td>\n",
       "      <td>2019</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numero                                              tweet  ...   año mes\n",
       "0       0  Acabo de sumarme a la #HoradelPlaneta con @WWF...  ...  2017   3\n",
       "1       1  \"Nos hemos quedado huérfanos de una fiscalía q...  ...  2016   3\n",
       "2       2   Acto Electoral en las #TorresdeCotillas Jueve...  ...  2019  11\n",
       "3       3  Para llevarse tan mal se entienden muy bien. P...  ...  2018   3\n",
       "4       4  #ComienzaElCambio en Andalucía tras cuatro déc...  ...  2019   1\n",
       "\n",
       "[5 rows x 9 columns]"
      ]
     },
     "execution_count": 342,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2jJOPfscAKQr"
   },
   "outputs": [],
   "source": [
    "#guardamos la version simplificada del df\n",
    "\n",
    "import csv\n",
    "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_arreglada.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"numero\", \"tweet\", \"nombre\", \"posicion\", \"cuenta\", \"fecha\",\"año\",\"mes\",\"tweet_limpios\"])\n",
    "    for i in range(len(df)):\n",
    "        writer.writerow([i, df['tweet'][i],df['nombre'][i],df['posicion'][i],df['cuenta'][i],df['fecha'][i],df['año'][i],df['mes'][i],df['tweet_limpios'][i]])\n",
    "#df.to_csv('data_set_arreglado_y_procesado.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BnAiNxL9G2Kz"
   },
   "source": [
    "## Simplificacion version 2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "executionInfo": {
     "elapsed": 203,
     "status": "error",
     "timestamp": 1643980974947,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "LOxpyL5aG1dV",
    "outputId": "478b814c-1ead-4eef-f245-78c8d3a42d99"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-cea384937d04>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# for showing a snapshot of the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/util/_decorators.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    309\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mstacklevel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m                 )\n\u001b[0;32m--> 311\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[1;32m    584\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 586\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    587\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    588\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    809\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"has_index_names\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    810\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 811\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    812\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    813\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1038\u001b[0m             )\n\u001b[1;32m   1039\u001b[0m         \u001b[0;31m# error: Too many arguments for \"ParserBase\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1040\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mmapping\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1041\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1042\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_failover_to_python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/c_parser_wrapper.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0;31m# open handles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_open_handles\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/parsers/base_parser.py\u001b[0m in \u001b[0;36m_open_handles\u001b[0;34m(self, src, kwds)\u001b[0m\n\u001b[1;32m    227\u001b[0m             \u001b[0mmemory_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"memory_map\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m             \u001b[0mstorage_options\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"storage_options\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 229\u001b[0;31m             \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"encoding_errors\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"strict\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    230\u001b[0m         )\n\u001b[1;32m    231\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    705\u001b[0m                 \u001b[0mencoding\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    706\u001b[0m                 \u001b[0merrors\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 707\u001b[0;31m                 \u001b[0mnewline\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    708\u001b[0m             )\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv')\n",
    "df.head(5) # for showing a snapshot of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 236
    },
    "executionInfo": {
     "elapsed": 314,
     "status": "error",
     "timestamp": 1643984660566,
     "user": {
      "displayName": "Michael Soughton",
      "photoUrl": "https://lh3.googleusercontent.com/a/default-user=s64",
      "userId": "02737434027374845423"
     },
     "user_tz": 0
    },
    "id": "9ISK_NevQ6Ec",
    "outputId": "6824d71e-12e3-4e33-e54a-5ed7348f08a9"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-c6b6aef5cb6f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     73\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 75\u001b[0;31m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_limpios'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclean_tweets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     76\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_limpios'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_limpios'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"[^a-zA-Z#áéíóúñ]\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\" \"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_limpios'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tweet_limpios'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m' '\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mw\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m>\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import clear_output\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "\n",
    "#limpiamos emoticonos y cosas varias que no son utiles para nuestro analisis\n",
    "\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)        \n",
    "    return input_txt\n",
    "\n",
    "def eliminaremotideunstring(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "def eliminaremoti(lista):\n",
    "    for i in range(len(lista)):\n",
    "        lista[i]=eliminaremotideunstring(lista[i])\n",
    "    return lista\n",
    "\n",
    "\n",
    "def clean_tweets(lst):\n",
    "    # eliminamos URLs (httpxxx)\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"https?://[A-Za-z0-9./]*\")\n",
    "    # eliminamos caracteres, numeros, signos depuntuacion (exceptuando #, los hastags son utiles)\n",
    "    lst = np.core.defchararray.replace(lst, \"[^a-zA-Z#]\", \" \")\n",
    "    # Eliminamos los emojis\n",
    "    lst = eliminaremoti(lst)\n",
    "    # eliminamos (@xxx)\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"[\\n]*\")\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"[:]*\")\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"@[\\w]*\")\n",
    "    \n",
    "    return lst\n",
    "\n",
    "\n",
    "# Defining a module for Text Processing\n",
    "def text_process(tex):\n",
    "    # 1. Eliminamos todas las cosas de marcacion: puntos, comas, etc\n",
    "    nopunct=[char for char in tex if char not in string.punctuation]\n",
    "    nopunct=''.join(nopunct)\n",
    "    \n",
    "    # 2. Lemmatisation: proceso de agrupacion de palabras segun la raiz, por ejemplo\n",
    "    #    studing y study en el fondo son la misma palabra\n",
    "    \n",
    "    a=''\n",
    "    i=0\n",
    "    for i in range(len(nopunct.split())):\n",
    "        b=lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
    "        a=a+b+' '\n",
    "        \n",
    "    # 3. Eliminamos todas las palabras cortas e inutiles: a, an, the... que no nos\n",
    "    #   dicen absolutamente nada de la forma de escribir del autor\n",
    "    return [word for word in a.split() if word.lower() not in stopwords.words('spanish')] #usamos español\n",
    "    #return [word for word in nopunct.split() if word.lower() not in stopwords.words('spanish')]\n",
    "\n",
    "\n",
    "\n",
    "df['tweet_limpios'] = clean_tweets(df['tweet'])\n",
    "df['tweet_limpios'] = df['tweet_limpios'].str.replace(\"[^a-zA-Z#áéíóúñ]\", \" \")\n",
    "df['tweet_limpios'] = df['tweet_limpios'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "print(df.head(5))\n",
    "\n",
    "\n",
    "for i in range(len(df['tweet_limpios'])):\n",
    "#for i in tqdm.tqdm(range(100)):\n",
    "    df['tweet_limpios'][i] = text_process(df['tweet_limpios'][i])\n",
    "    df['tweet_limpios'][i] = ' '.join(df['tweet_limpios'][i])\n",
    "    clear_output(wait=True)\n",
    "    print(i, \"/\", len(df['tweet_limpios']), sep=\"\")   \n",
    "    \n",
    "\n",
    "    # or other long operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dFHyGN-aQ6Bs"
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "#from datetime import datetime\n",
    "#from dateutil.parser import parse\n",
    "\n",
    "año = []\n",
    "mes = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    año.append(parse(df['fecha'][i]).year)\n",
    "    mes.append(parse(df['fecha'][i]).month)\n",
    "df['año'] = año\n",
    "df['mes'] = mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "e9A6sPeiE7Dy"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_super_arreglada.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"numero\", \"tweet\", \"nombre\", \"posicion\", \"cuenta\", \"fecha\",\"año\",\"mes\",\"tweet_limpios\"])\n",
    "    for i in range(len(df)):\n",
    "        writer.writerow([i, df['tweet'][i],df['nombre'][i],df['posicion'][i],df['cuenta'][i],df['fecha'][i],df['año'][i],df['mes'][i],df['tweet_limpios'][i]])\n",
    "#df.to_csv('data_set_arreglado_y_procesado.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5sdEUpflE7rK"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BtQrncV7dFCu"
   },
   "source": [
    "## Simplificación Versión 3 (sin Hastags, no utilizada)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 204
    },
    "executionInfo": {
     "elapsed": 2043,
     "status": "ok",
     "timestamp": 1591527979386,
     "user": {
      "displayName": "Miguel Garcia Folgado",
      "photoUrl": "",
      "userId": "03418710468114118723"
     },
     "user_tz": -120
    },
    "id": "LR-OO-hyE7pN",
    "outputId": "e3eb40e9-deef-460b-d50d-a36bf94e84e6"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>numero</th>\n",
       "      <th>tweet</th>\n",
       "      <th>nombre</th>\n",
       "      <th>posicion</th>\n",
       "      <th>cuenta</th>\n",
       "      <th>fecha</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Acabo de sumarme a la #HoradelPlaneta con @WWF...</td>\n",
       "      <td>PSOE</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@JoseantonioJun</td>\n",
       "      <td>2017-03-22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>\"Nos hemos quedado huérfanos de una fiscalía q...</td>\n",
       "      <td>POD</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@anamarcelloana</td>\n",
       "      <td>2016-03-04</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>Acto Electoral en las #TorresdeCotillas Jueve...</td>\n",
       "      <td>VOX</td>\n",
       "      <td>OPO</td>\n",
       "      <td>@Vox_Molina</td>\n",
       "      <td>2019-11-05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>Para llevarse tan mal se entienden muy bien. P...</td>\n",
       "      <td>PSOE</td>\n",
       "      <td>GOV</td>\n",
       "      <td>@sanchezcastejon</td>\n",
       "      <td>2018-03-26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>#ComienzaElCambio en Andalucía tras cuatro déc...</td>\n",
       "      <td>PP</td>\n",
       "      <td>OPO</td>\n",
       "      <td>@pablocasado_</td>\n",
       "      <td>2019-01-16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   numero  ...       fecha\n",
       "0       0  ...  2017-03-22\n",
       "1       1  ...  2016-03-04\n",
       "2       2  ...  2019-11-05\n",
       "3       3  ...  2018-03-26\n",
       "4       4  ...  2019-01-16\n",
       "\n",
       "[5 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_sin_nada_reducido.csv')\n",
    "df.head(5) # for showing a snapshot of the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 153
    },
    "executionInfo": {
     "elapsed": 2846216,
     "status": "ok",
     "timestamp": 1591530827853,
     "user": {
      "displayName": "Miguel Garcia Folgado",
      "photoUrl": "",
      "userId": "03418710468114118723"
     },
     "user_tz": -120
    },
    "id": "IZhaBVrME7dr",
    "outputId": "0260f53f-df01-4597-880e-d60b9b8777ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138202/138203\n"
     ]
    }
   ],
   "source": [
    "# Importing necessary libraries\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from IPython.display import clear_output\n",
    "lemmatiser = WordNetLemmatizer()\n",
    "\n",
    "import tqdm\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import re\n",
    "def remove_pattern(input_txt, pattern):\n",
    "    r = re.findall(pattern, input_txt)\n",
    "    for i in r:\n",
    "        input_txt = re.sub(i, '', input_txt)        \n",
    "    return input_txt\n",
    "\n",
    "def eliminaremotideunstring(string):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           u\"\\U00002702-\\U000027B0\"\n",
    "                           u\"\\U000024C2-\\U0001F251\"\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    return emoji_pattern.sub(r'', string)\n",
    "def eliminaremoti(lista):\n",
    "    for i in range(len(lista)):\n",
    "        lista[i]=eliminaremotideunstring(lista[i])\n",
    "    return lista\n",
    "\n",
    "\n",
    "def clean_tweets(lst):\n",
    "    # remove twitter Return handles (RT @xxx:)\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"RT @[\\w]*:\")\n",
    "    # remove twitter handles (@xxx)\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"@[\\w]*\")\n",
    "    # remove twitter hastags (@xxx)\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"#[\\w]*\")\n",
    "    # remove URL links (httpxxx)\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"https?://[A-Za-z0-9./]*\")\n",
    "    # remove special characters, numbers, punctuations (except for #)\n",
    "    lst = np.core.defchararray.replace(lst, \"[^a-zA-Z#]\", \" \")\n",
    "    # Eliminamos los emojis\n",
    "    lst = eliminaremoti(lst)\n",
    "    # remove twitter handles (@xxx)\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"[\\n]*\")\n",
    "    lst = np.vectorize(remove_pattern)(lst, \"[:]*\")\n",
    "    \n",
    "    return lst\n",
    "\n",
    "\n",
    "# Defining a module for Text Processing\n",
    "def text_process(tex):\n",
    "    # 1. Eliminamos todas las cosas de marcacion, rollo puntos, comas, etc\n",
    "    nopunct=[char for char in tex if char not in string.punctuation]\n",
    "    nopunct=''.join(nopunct)\n",
    "    \n",
    "    # 2. Lemmatisation: proceso de agrupacion de palabras segun la raiz, por ejemplo\n",
    "    #    studing y study en el fondo son la misma palabra\n",
    "    \n",
    "    a=''\n",
    "    i=0\n",
    "    for i in range(len(nopunct.split())):\n",
    "        b=lemmatiser.lemmatize(nopunct.split()[i], pos=\"v\")\n",
    "        a=a+b+' '\n",
    "        \n",
    "    # 3. Eliminamos todas las palabras cortas e inutiles, rollo a, an, the... que no nos\n",
    "    #   dicen absolutamente nada de la forma de escribir del autor\n",
    "    return [word for word in a.split() if word.lower() not in stopwords.words('spanish')]\n",
    "    #return [word for word in nopunct.split() if word.lower() not in stopwords.words('spanish')]\n",
    "\n",
    "\n",
    "\n",
    "df['tweet_limpios'] = clean_tweets(df['tweet'])\n",
    "df['tweet_limpios'] = df['tweet_limpios'].str.replace(\"[^a-zA-Z#áéíóúñ]\", \" \")\n",
    "df['tweet_limpios'] = df['tweet_limpios'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "print(df.head(5))\n",
    "\n",
    "\n",
    "for i in range(len(df['tweet_limpios'])):\n",
    "#for i in tqdm.tqdm(range(100)):\n",
    "    df['tweet_limpios'][i] = text_process(df['tweet_limpios'][i])\n",
    "    df['tweet_limpios'][i] = ' '.join(df['tweet_limpios'][i])\n",
    "    clear_output(wait=True)\n",
    "    print(i, \"/\", len(df['tweet_limpios']), sep=\"\")   \n",
    "    \n",
    "\n",
    "    # or other long operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WUPSC0r6E7bj"
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse\n",
    "#from datetime import datetime\n",
    "#from dateutil.parser import parse\n",
    "\n",
    "año = []\n",
    "mes = []\n",
    "\n",
    "for i in range(len(df)):\n",
    "    año.append(parse(df['fecha'][i]).year)\n",
    "    mes.append(parse(df['fecha'][i]).month)\n",
    "df['año'] = año\n",
    "df['mes'] = mes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4flWfjAE7XF"
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "with open('/content/gdrive/My Drive/proyecto_machine_learning/tweets_definitivo_hasta_2016_version_super_arreglada_sin_hashtags.csv', 'w', newline='') as file:\n",
    "    writer = csv.writer(file)\n",
    "    writer.writerow([\"numero\", \"tweet\", \"nombre\", \"posicion\", \"cuenta\", \"fecha\",\"año\",\"mes\",\"tweet_limpios\"])\n",
    "    for i in range(len(df)):\n",
    "        writer.writerow([i, df['tweet'][i],df['nombre'][i],df['posicion'][i],df['cuenta'][i],df['fecha'][i],df['año'][i],df['mes'][i],df['tweet_limpios'][i]])\n",
    "#df.to_csv('data_set_arreglado_y_procesado.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "_3kF5Bdn3Ao1",
    "A1VZ5lebPoTd"
   ],
   "name": "Copy of scraping_version_compartirda_nuevo_proyecto_NLP.ipynb",
   "provenance": [
    {
     "file_id": "1lLwpmDaptOM7NW0jC0jQ-x3LTNQUNIMY",
     "timestamp": 1643980179055
    },
    {
     "file_id": "1frt8SQYJUpyAIACDFItsUZFcAHmMozg0",
     "timestamp": 1643973793694
    },
    {
     "file_id": "10dh42YFEwDc1rJ19lMMW7vl9FaAPIlfq",
     "timestamp": 1629283032852
    }
   ]
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
